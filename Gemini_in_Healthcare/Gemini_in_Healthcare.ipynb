{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudio1975/Medium-blog/blob/master/Gemini_in_Healthcare/Gemini_in_Healthcare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d148c043",
      "metadata": {
        "papermill": {
          "duration": 0.020494,
          "end_time": "2024-11-30T20:14:16.764295",
          "exception": false,
          "start_time": "2024-11-30T20:14:16.743801",
          "status": "completed"
        },
        "tags": [],
        "id": "d148c043"
      },
      "source": [
        "# Competition"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64473a51",
      "metadata": {
        "papermill": {
          "duration": 0.019723,
          "end_time": "2024-11-30T20:14:16.803635",
          "exception": false,
          "start_time": "2024-11-30T20:14:16.783912",
          "status": "completed"
        },
        "tags": [],
        "id": "64473a51"
      },
      "source": [
        "The competition's goal is to explore and demonstrate innovative use cases for the Gemini 1.5 model's huge context window, which can process up to 2 million tokens at once. Participants are encouraged to create public Kaggle Notebooks and YouTube Videos showcasing how various applications can leverage this capability. The competition aims to highlight the potential of long context windows in AI, allowing for more advanced and direct methods like in-context retrieval and extensive many-shot prompting and invites participants to \"stress test\" the model by applying it to creative and challenging scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9da7f75",
      "metadata": {
        "papermill": {
          "duration": 0.021938,
          "end_time": "2024-11-30T20:14:16.849309",
          "exception": false,
          "start_time": "2024-11-30T20:14:16.827371",
          "status": "completed"
        },
        "tags": [],
        "id": "b9da7f75"
      },
      "source": [
        "# Gemini 1.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03795f0e",
      "metadata": {
        "papermill": {
          "duration": 0.021611,
          "end_time": "2024-11-30T20:14:16.892910",
          "exception": false,
          "start_time": "2024-11-30T20:14:16.871299",
          "status": "completed"
        },
        "tags": [],
        "id": "03795f0e"
      },
      "source": [
        "On 8th of August 2024, Google published a report https://arxiv.org/pdf/2403.05530 to introduce the Gemini 1.5, a family of multimodal models boasting exceptional compute efficiency and the ability to recall and reason over intricate details from extensive contexts spanning millions of tokens. This includes the processing of long documents, hours of video and audio content. Gemini 1.5 comprises two models: Gemini 1.5 Pro, an improved version with enhanced capabilities, and Gemini 1.5 Flash, a lightweight variant prioritising efficiency with minimal quality compromise. A key advancement is the models' ability to achieve near-perfect recall in long-context retrieval tasks across modalities, exceeding the capabilities of existing models like Claude 3.0 and GPT-4 Turbo.nts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df241dfd",
      "metadata": {
        "papermill": {
          "duration": 0.019781,
          "end_time": "2024-11-30T20:14:16.932742",
          "exception": false,
          "start_time": "2024-11-30T20:14:16.912961",
          "status": "completed"
        },
        "tags": [],
        "id": "df241dfd"
      },
      "source": [
        "# Use Case: A Comprehensive Review of Generative AI in Healthcare"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c24f6222",
      "metadata": {
        "papermill": {
          "duration": 0.018986,
          "end_time": "2024-11-30T20:14:17.008344",
          "exception": false,
          "start_time": "2024-11-30T20:14:16.989358",
          "status": "completed"
        },
        "tags": [],
        "id": "c24f6222"
      },
      "source": [
        "This review paper examines the applications of generative AI models, specifically transformers and diffusion models, in the healthcare sector. The authors argue that these models have revolutionized the analysis of various data forms, including medical imaging, protein structure prediction, clinical documentation, and drug desig . They present a novel classification of generative AI models in healthcare: diffusion models and transformer-based modeL They further categorize the uses of these models based on specific healthcare tasks. For example:\n",
        "Transformer-based models have been used for tasks such as:\n",
        "Protein structure prediction.\n",
        "Clinical documentation and information extraction.\n",
        "Diagnostic assistance.\n",
        "Medical imaging and radiology interpretation.\n",
        "Clinical decision support.\n",
        "Medical coding and billing.\n",
        "Drug design and molecular representation.\n",
        "Diffusion models have been used for tasks such as:\n",
        "Image reconstruction.\n",
        "Image-to-image translation.\n",
        "Image generation.\n",
        "Image classification.\n",
        "The review argues that generative AI has the potential to become a highly trustworthy tool in healthcare, potentially replacing human doctors in certain tasks.\n",
        "https://arxiv.org/abs/2310.00795"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7ee3f47",
      "metadata": {
        "papermill": {
          "duration": 0.018673,
          "end_time": "2024-11-30T20:14:17.046191",
          "exception": false,
          "start_time": "2024-11-30T20:14:17.027518",
          "status": "completed"
        },
        "tags": [],
        "id": "c7ee3f47"
      },
      "source": [
        "# Installation of Required Packages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "215c7795",
      "metadata": {
        "papermill": {
          "duration": 0.018557,
          "end_time": "2024-11-30T20:14:17.083425",
          "exception": false,
          "start_time": "2024-11-30T20:14:17.064868",
          "status": "completed"
        },
        "tags": [],
        "id": "215c7795"
      },
      "source": [
        "pip command to install Python packages required for various tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90705ccf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:14:17.122855Z",
          "iopub.status.busy": "2024-11-30T20:14:17.122105Z",
          "iopub.status.idle": "2024-11-30T20:14:27.259906Z",
          "shell.execute_reply": "2024-11-30T20:14:27.258701Z"
        },
        "papermill": {
          "duration": 10.16014,
          "end_time": "2024-11-30T20:14:27.262219",
          "exception": false,
          "start_time": "2024-11-30T20:14:17.102079",
          "status": "completed"
        },
        "tags": [],
        "id": "90705ccf"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers &>/dev/null # for sentence embeddings and similarity computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c56c423",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:14:27.302353Z",
          "iopub.status.busy": "2024-11-30T20:14:27.302049Z",
          "iopub.status.idle": "2024-11-30T20:14:46.316127Z",
          "shell.execute_reply": "2024-11-30T20:14:46.314835Z"
        },
        "papermill": {
          "duration": 19.037072,
          "end_time": "2024-11-30T20:14:46.318968",
          "exception": false,
          "start_time": "2024-11-30T20:14:27.281896",
          "status": "completed"
        },
        "tags": [],
        "id": "2c56c423"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers &>/dev/null # for Autotokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef577af3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:14:46.369772Z",
          "iopub.status.busy": "2024-11-30T20:14:46.369409Z",
          "iopub.status.idle": "2024-11-30T20:14:54.945878Z",
          "shell.execute_reply": "2024-11-30T20:14:54.944922Z"
        },
        "papermill": {
          "duration": 8.599532,
          "end_time": "2024-11-30T20:14:54.948033",
          "exception": false,
          "start_time": "2024-11-30T20:14:46.348501",
          "status": "completed"
        },
        "tags": [],
        "id": "ef577af3"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2 &>/dev/null # for reading PDF files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e656682",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:14:54.989348Z",
          "iopub.status.busy": "2024-11-30T20:14:54.989067Z",
          "iopub.status.idle": "2024-11-30T20:15:05.457641Z",
          "shell.execute_reply": "2024-11-30T20:15:05.456689Z"
        },
        "papermill": {
          "duration": 10.491521,
          "end_time": "2024-11-30T20:15:05.459703",
          "exception": false,
          "start_time": "2024-11-30T20:14:54.968182",
          "status": "completed"
        },
        "tags": [],
        "id": "6e656682"
      },
      "outputs": [],
      "source": [
        "!pip install rouge-score &>/dev/null # for evaluating the similarity of texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c67d313",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:15:05.500098Z",
          "iopub.status.busy": "2024-11-30T20:15:05.499792Z",
          "iopub.status.idle": "2024-11-30T20:15:14.163786Z",
          "shell.execute_reply": "2024-11-30T20:15:14.162568Z"
        },
        "papermill": {
          "duration": 8.686446,
          "end_time": "2024-11-30T20:15:14.166012",
          "exception": false,
          "start_time": "2024-11-30T20:15:05.479566",
          "status": "completed"
        },
        "tags": [],
        "id": "3c67d313"
      },
      "outputs": [],
      "source": [
        "!pip install bert-score &>/dev/null # for evaluating the similarity of texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93eb6276",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:15:14.207685Z",
          "iopub.status.busy": "2024-11-30T20:15:14.207359Z",
          "iopub.status.idle": "2024-11-30T20:15:23.942850Z",
          "shell.execute_reply": "2024-11-30T20:15:23.941602Z"
        },
        "papermill": {
          "duration": 9.757906,
          "end_time": "2024-11-30T20:15:23.945050",
          "exception": false,
          "start_time": "2024-11-30T20:15:14.187144",
          "status": "completed"
        },
        "tags": [],
        "id": "93eb6276"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu &>/dev/null # for efficient similarity searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3205caea",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:15:23.986956Z",
          "iopub.status.busy": "2024-11-30T20:15:23.986177Z",
          "iopub.status.idle": "2024-11-30T20:15:32.780415Z",
          "shell.execute_reply": "2024-11-30T20:15:32.779460Z"
        },
        "papermill": {
          "duration": 8.818037,
          "end_time": "2024-11-30T20:15:32.783129",
          "exception": false,
          "start_time": "2024-11-30T20:15:23.965092",
          "status": "completed"
        },
        "tags": [],
        "id": "3205caea"
      },
      "outputs": [],
      "source": [
        "!pip install ipywidgets &>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e617cd4e",
      "metadata": {
        "papermill": {
          "duration": 0.019101,
          "end_time": "2024-11-30T20:15:32.822814",
          "exception": false,
          "start_time": "2024-11-30T20:15:32.803713",
          "status": "completed"
        },
        "tags": [],
        "id": "e617cd4e"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42138241",
      "metadata": {
        "papermill": {
          "duration": 0.018924,
          "end_time": "2024-11-30T20:15:32.860886",
          "exception": false,
          "start_time": "2024-11-30T20:15:32.841962",
          "status": "completed"
        },
        "tags": [],
        "id": "42138241"
      },
      "source": [
        "Imports various libraries needed for specific tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8be9f0e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:15:32.940358Z",
          "iopub.status.busy": "2024-11-30T20:15:32.939437Z",
          "iopub.status.idle": "2024-11-30T20:15:56.790110Z",
          "shell.execute_reply": "2024-11-30T20:15:56.789306Z"
        },
        "papermill": {
          "duration": 23.874221,
          "end_time": "2024-11-30T20:15:56.792185",
          "exception": false,
          "start_time": "2024-11-30T20:15:32.917964",
          "status": "completed"
        },
        "tags": [],
        "id": "a8be9f0e"
      },
      "outputs": [],
      "source": [
        "# Get the API key from here: https://ai.google.dev/tutorials/setup\n",
        "# Create a new secret called \"GEMINI_API_KEY\" via Add-ons -> Secrets in the top menu, and attach it to this notebook.\n",
        "from kaggle_secrets import UserSecretsClient # for accessing stored secrets in Kaggle (like API keys).\n",
        "import os # File-system operations\n",
        "import PyPDF2 # reading PDFs\n",
        "from IPython.display import display, Markdown # displaying Markdown\n",
        "from rouge_score import rouge_scorer # Scoring functions (ROUGE and BERT) for text evaluation\n",
        "from bert_score import score as bert_score # Scoring functions (ROUGE and BERT) for text evaluation\n",
        "import faiss # for handling and searching text data\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # for handling and searching text data\n",
        "from sentence_transformers import SentenceTransformer, util # for obtaining sentence embeddings\n",
        "from transformers import AutoTokenizer # For counting tokens\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# This suppresses any future warnings, which can be useful for avoiding clutter in the output.\n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\", FutureWarning)\n",
        "\n",
        "# Set up access to the GEMINI API\n",
        "user_secrets = UserSecretsClient()\n",
        "apiKey = user_secrets.get_secret(\"GEMINI_API_KEY\") # Retrieve the API key from the Kaggle secrets.\n",
        "import google.generativeai as genai\n",
        "genai.configure(api_key = apiKey) # Configure the genai library with the API key to access GEMINI's models.\n",
        "llm = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash-latest\") # Initialize a generative model for use in later queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3bf019f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:15:56.833156Z",
          "iopub.status.busy": "2024-11-30T20:15:56.832561Z",
          "iopub.status.idle": "2024-11-30T20:15:56.842885Z",
          "shell.execute_reply": "2024-11-30T20:15:56.841893Z"
        },
        "papermill": {
          "duration": 0.032478,
          "end_time": "2024-11-30T20:15:56.844517",
          "exception": false,
          "start_time": "2024-11-30T20:15:56.812039",
          "status": "completed"
        },
        "tags": [],
        "id": "c3bf019f",
        "outputId": "74281407-b6de-4d45-c14d-12dc27619a55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/gemini-long-context/submission_instructions.txt\n",
            "/kaggle/input/ai-in-healthcare/Figure_14.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_10.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_1.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_8.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_9.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_11.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_15.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_2.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_5.pdf\n",
            "/kaggle/input/ai-in-healthcare/Figure_13.pdf\n",
            "/kaggle/input/ai-in-healthcare/2310.00795v1.pdf\n"
          ]
        }
      ],
      "source": [
        "# Walks the input directory (/kaggle/input) to find files, specifically PDF paths.\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9473d138",
      "metadata": {
        "papermill": {
          "duration": 0.019095,
          "end_time": "2024-11-30T20:15:56.883228",
          "exception": false,
          "start_time": "2024-11-30T20:15:56.864133",
          "status": "completed"
        },
        "tags": [],
        "id": "9473d138"
      },
      "source": [
        "### Reading document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc006cf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:15:56.924083Z",
          "iopub.status.busy": "2024-11-30T20:15:56.923413Z",
          "iopub.status.idle": "2024-11-30T20:15:58.803844Z",
          "shell.execute_reply": "2024-11-30T20:15:58.802936Z"
        },
        "papermill": {
          "duration": 1.90256,
          "end_time": "2024-11-30T20:15:58.805766",
          "exception": false,
          "start_time": "2024-11-30T20:15:56.903206",
          "status": "completed"
        },
        "tags": [],
        "id": "9dc006cf",
        "outputId": "3f6e55b7-c31c-4d83-e7f6-a677b9394ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully extracted text from: /kaggle/input/ai-in-healthcare/2310.00795v1.pdf\n"
          ]
        }
      ],
      "source": [
        "file_paths = ['/kaggle/input/ai-in-healthcare/2310.00795v1.pdf']\n",
        "\n",
        "# Extracts text from each PDF using PyPDF2 and stores text in papers_text.\n",
        "papers_text = []  # List to store the extracted text from papers\n",
        "# Extract text from PDFs\n",
        "for file_path in file_paths:\n",
        "    try:\n",
        "        with open(file_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "            papers_text.append(text)\n",
        "        print(f\"Successfully extracted text from: {file_path}\") # Handles any exceptions that occur during the extraction process and logs success or failure messages.\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract text from {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3eaeba4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:15:58.846620Z",
          "iopub.status.busy": "2024-11-30T20:15:58.846002Z",
          "iopub.status.idle": "2024-11-30T20:16:00.028558Z",
          "shell.execute_reply": "2024-11-30T20:16:00.027522Z"
        },
        "papermill": {
          "duration": 1.205473,
          "end_time": "2024-11-30T20:16:00.031375",
          "exception": false,
          "start_time": "2024-11-30T20:15:58.825902",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "9e7fe2e97c244d77b0827a83c7f7bc1f",
            "5821fdd695fb4f0b990bb99902535172",
            "d383f2b35b2b4982875d5e1dcf3189fb",
            "b16f7a64207a4ef29c788249d690143e"
          ]
        },
        "id": "a3eaeba4",
        "outputId": "c6598f4a-6f67-4546-babb-8504f4b2f05b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e7fe2e97c244d77b0827a83c7f7bc1f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5821fdd695fb4f0b990bb99902535172",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d383f2b35b2b4982875d5e1dcf3189fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b16f7a64207a4ef29c788249d690143e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (35853 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of tokens: 35853\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer for a specific model, e.g., BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "total_tokens = 0\n",
        "\n",
        "for paper in papers_text:\n",
        "    # Tokenize the paper's text and count the tokens\n",
        "    tokens = tokenizer.tokenize(paper)\n",
        "    total_tokens += len(tokens)\n",
        "\n",
        "print(f\"Total number of tokens: {total_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6edd98b4",
      "metadata": {
        "papermill": {
          "duration": 0.02131,
          "end_time": "2024-11-30T20:16:00.077933",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.056623",
          "status": "completed"
        },
        "tags": [],
        "id": "6edd98b4"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d3ef00",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:00.121094Z",
          "iopub.status.busy": "2024-11-30T20:16:00.120429Z",
          "iopub.status.idle": "2024-11-30T20:16:00.125062Z",
          "shell.execute_reply": "2024-11-30T20:16:00.124128Z"
        },
        "papermill": {
          "duration": 0.027966,
          "end_time": "2024-11-30T20:16:00.126858",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.098892",
          "status": "completed"
        },
        "tags": [],
        "id": "22d3ef00"
      },
      "outputs": [],
      "source": [
        "# Define a set of questions to ask regarding the paper's content.\n",
        "question_1 = \"Which types of encoder-decoder transformer architectures are discussed in the review ?\"\n",
        "question_2 = \"Could you say who developed MERGIS, and what is it ?\"\n",
        "question_3 = \"Could you say who developed AdaDiff, and what is it  ?\"\n",
        "question_4 = \"What process represents the equation 1 used in the review, could you explain it ?\"\n",
        "question_5 = \"Could you say who developed ProteinBERT, and what is it ?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ef5b7d7",
      "metadata": {
        "papermill": {
          "duration": 0.020862,
          "end_time": "2024-11-30T20:16:00.169128",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.148266",
          "status": "completed"
        },
        "tags": [],
        "id": "1ef5b7d7"
      },
      "source": [
        "### Scoring Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e369758f",
      "metadata": {
        "papermill": {
          "duration": 0.020801,
          "end_time": "2024-11-30T20:16:00.210857",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.190056",
          "status": "completed"
        },
        "tags": [],
        "id": "e369758f"
      },
      "source": [
        "ROUGE scores: Measures overlap between a generated and reference text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e6feadb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:00.253488Z",
          "iopub.status.busy": "2024-11-30T20:16:00.253215Z",
          "iopub.status.idle": "2024-11-30T20:16:00.258150Z",
          "shell.execute_reply": "2024-11-30T20:16:00.257444Z"
        },
        "papermill": {
          "duration": 0.028297,
          "end_time": "2024-11-30T20:16:00.259748",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.231451",
          "status": "completed"
        },
        "tags": [],
        "id": "4e6feadb"
      },
      "outputs": [],
      "source": [
        "def calculate_rouge_scores(reference_text, generated_text):\n",
        "    # Initialize the ROUGE scorer with the desired metrics\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    # Calculate ROUGE scores\n",
        "    scores = scorer.score(reference_text, generated_text)\n",
        "    # Display or return the results\n",
        "    print(\"ROUGE Scores:\")\n",
        "    for key, value in scores.items():\n",
        "        print(f\"{key}: precision={value.precision:.4f}, recall={value.recall:.4f}, fmeasure={value.fmeasure:.4f}\")\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a92bfe21",
      "metadata": {
        "papermill": {
          "duration": 0.019676,
          "end_time": "2024-11-30T20:16:00.298870",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.279194",
          "status": "completed"
        },
        "tags": [],
        "id": "a92bfe21"
      },
      "source": [
        "BERT score: Uses a pre-trained BERT model to evaluate the semantic similarity between texts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188057b5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:00.339707Z",
          "iopub.status.busy": "2024-11-30T20:16:00.339410Z",
          "iopub.status.idle": "2024-11-30T20:16:00.343942Z",
          "shell.execute_reply": "2024-11-30T20:16:00.343151Z"
        },
        "papermill": {
          "duration": 0.027279,
          "end_time": "2024-11-30T20:16:00.345557",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.318278",
          "status": "completed"
        },
        "tags": [],
        "id": "188057b5"
      },
      "outputs": [],
      "source": [
        "def calculate_bert_score(reference_text, generated_text):\n",
        "    precision, recall, f1 = bert_score([generated_text], [reference_text], model_type=\"bert-base-multilingual-cased\",lang=\"en\")\n",
        "    print(f\"BERTScore: Precision={precision.mean().item():.4f}, Recall={recall.mean().item():.4f}, F1={f1.mean().item():.4f}\")\n",
        "    return precision.mean().item(), recall.mean().item(), f1.mean().item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a95d1d",
      "metadata": {
        "papermill": {
          "duration": 0.019783,
          "end_time": "2024-11-30T20:16:00.384962",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.365179",
          "status": "completed"
        },
        "tags": [],
        "id": "d1a95d1d"
      },
      "source": [
        "### Gemini context and task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "165a63bc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:00.424953Z",
          "iopub.status.busy": "2024-11-30T20:16:00.424665Z",
          "iopub.status.idle": "2024-11-30T20:16:00.428527Z",
          "shell.execute_reply": "2024-11-30T20:16:00.427696Z"
        },
        "papermill": {
          "duration": 0.02591,
          "end_time": "2024-11-30T20:16:00.430279",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.404369",
          "status": "completed"
        },
        "tags": [],
        "id": "165a63bc"
      },
      "outputs": [],
      "source": [
        "# This context is used to frame the interaction with the generative model, providing a perspective or role to shape responses.\n",
        "context_info = \"\"\" You are a Machine Learning research scientist with more than\n",
        "15 years of experience. Your role is to retrieve and analyse information from a\n",
        "paper about AI in healthcare topic. Please provide only information from the paper.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94e3b6bb",
      "metadata": {
        "papermill": {
          "duration": 0.019965,
          "end_time": "2024-11-30T20:16:00.470527",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.450562",
          "status": "completed"
        },
        "tags": [],
        "id": "94e3b6bb"
      },
      "source": [
        "# Model 1: knowledge base Gemini 1.5 Flash"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83de53c",
      "metadata": {
        "papermill": {
          "duration": 0.019507,
          "end_time": "2024-11-30T20:16:00.510002",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.490495",
          "status": "completed"
        },
        "tags": [],
        "id": "b83de53c"
      },
      "source": [
        "Gemini 1.5 Flash is a smaller and more efficient version of Gemini 1.5 Pro, designed for faster performance and lower latency. Here is a summary of its features:\n",
        "Model Architecture: Unlike the sparse mixture-of-experts (MoE) architecture of Gemini 1.5 Pro, Flash is a dense transformer decoder model. This design choice, along with techniques like parallel computation of attention and feedforward components, contributes to its efficiency and lower latency.\n",
        "Distillation from Gemini 1.5 Pro: Gemini 1.5 Flash is trained through online distillation from the larger Gemini 1.5 Pro model. This means it learns from the knowledge and capabilities of the more powerful model, achieving comparable performance while maintaining a smaller size.\n",
        "Long-Context Handling: Like Gemini 1.5 Pro, Flash can handle a context window of over 2 million tokens. This allows it to process and recall information from very long inputs, including extensive documents, hours of video, and days of audio.\n",
        "Multimodality: Gemini 1.5 Flash inherits the multimodal capabilities of its larger counterpart, enabling it to process and understand various data types, including text, images, audio, and video. This makes it suitable for tasks requiring comprehension and reasoning across different modalities.\n",
        "Serving Efficiency and Latency: One of the most prominent features of Gemini 1.5 Flash is its fast inference time. It consistently demonstrates faster output generation than other leading large language models, including GPT-3.5 Turbo, GPT-4 Turbo, and Claude 3, across languages like English, Japanese, Chinese, and French.\n",
        "Performance: While smaller than Gemini 1.5 Pro, Flash maintains high performance across various tasks. It achieves near-perfect recall in long-context retrieval tasks across modalities, excels in long-document and long-video question answering, and matches or surpasses Gemini 1.0 Ultra's performance on several benchmarks.\n",
        "In-Context Learning: The long-context capabilities of Gemini 1.5 Flash also enhance its in-context learning ability. This is exemplified by its performance in low-resource language translation, where it shows continuous improvement as the number of in-context examples increases.\n",
        "Safety and Security: Gemini 1.5 Flash demonstrates significant improvements in safety and security compared to previous models. It shows a substantial decrease in policy violation rates and improved robustness against jailbreak attempts.\n",
        "Overall, Gemini 1.5 Flash offers a compelling combination of efficiency, long-context understanding, and multimodal capabilities. It excels in tasks that benefit from its fast inference time and ability to process extensive inputs, making it a valuable tool for a wide range of applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3241102c",
      "metadata": {
        "papermill": {
          "duration": 0.019399,
          "end_time": "2024-11-30T20:16:00.549158",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.529759",
          "status": "completed"
        },
        "tags": [],
        "id": "3241102c"
      },
      "source": [
        "### Step 1: Implementing the Query System"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a5fa28",
      "metadata": {
        "papermill": {
          "duration": 0.020544,
          "end_time": "2024-11-30T20:16:00.589580",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.569036",
          "status": "completed"
        },
        "tags": [],
        "id": "02a5fa28"
      },
      "source": [
        "These functions manage conversations with the generative model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "064bfc7a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:00.633504Z",
          "iopub.status.busy": "2024-11-30T20:16:00.632578Z",
          "iopub.status.idle": "2024-11-30T20:16:00.639565Z",
          "shell.execute_reply": "2024-11-30T20:16:00.638679Z"
        },
        "papermill": {
          "duration": 0.030915,
          "end_time": "2024-11-30T20:16:00.641335",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.610420",
          "status": "completed"
        },
        "tags": [],
        "id": "064bfc7a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# start_chat_session: Starts a chat by combining a context and the text of the document\n",
        "def start_chat_session(context_info, papers_text):\n",
        "    if papers_text:\n",
        "        # Combine context with the text content of the first document\n",
        "        combined_content = context_info + \"\\n\\n\" + papers_text[0]\n",
        "\n",
        "        # Use the combined text in the model history\n",
        "        chat_session = llm.start_chat(\n",
        "            history=[\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'parts': [combined_content]\n",
        "                }\n",
        "            ]\n",
        "        )\n",
        "        return chat_session\n",
        "    return None\n",
        "\n",
        "# chatAI: Sends prompts to the chat session and returns the model's response\n",
        "def chatAI(chat_session, context_info, prompt):\n",
        "    if chat_session:\n",
        "        # Combine the prompt with context to provide more depth\n",
        "        full_prompt = f\"{context_info}\\n\\n{prompt}\"\n",
        "        response = chat_session.send_message(full_prompt)\n",
        "        return response.text\n",
        "    return \"No chat session initialised.\"\n",
        "\n",
        "# The chat session is initiated using the extracted paper text and context.\n",
        "chat_session = start_chat_session(context_info, papers_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e90980b",
      "metadata": {
        "papermill": {
          "duration": 0.020159,
          "end_time": "2024-11-30T20:16:00.681847",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.661688",
          "status": "completed"
        },
        "tags": [],
        "id": "3e90980b"
      },
      "source": [
        "### Step 2: Asking Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d282428",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:00.723012Z",
          "iopub.status.busy": "2024-11-30T20:16:00.722478Z",
          "iopub.status.idle": "2024-11-30T20:16:16.164794Z",
          "shell.execute_reply": "2024-11-30T20:16:16.164045Z"
        },
        "papermill": {
          "duration": 15.464664,
          "end_time": "2024-11-30T20:16:16.166746",
          "exception": false,
          "start_time": "2024-11-30T20:16:00.702082",
          "status": "completed"
        },
        "tags": [],
        "id": "1d282428"
      },
      "outputs": [],
      "source": [
        "# Each question is sent to the model, and its responses are captured and stored.\n",
        "response_1_base=chatAI(chat_session, context_info,question_1)\n",
        "response_2_base=chatAI(chat_session, context_info,question_2)\n",
        "response_3_base=chatAI(chat_session, context_info,question_3)\n",
        "response_4_base=chatAI(chat_session, context_info,question_4)\n",
        "response_5_base=chatAI(chat_session, context_info,question_5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "618d0b0d",
      "metadata": {
        "papermill": {
          "duration": 0.01939,
          "end_time": "2024-11-30T20:16:16.207252",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.187862",
          "status": "completed"
        },
        "tags": [],
        "id": "618d0b0d"
      },
      "source": [
        "# What is a Retrieval-Augmented Generation ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c752ac85",
      "metadata": {
        "papermill": {
          "duration": 0.019279,
          "end_time": "2024-11-30T20:16:16.245889",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.226610",
          "status": "completed"
        },
        "tags": [],
        "id": "c752ac85"
      },
      "source": [
        "Retrieval-Augmented Generation (RAG) enhances the capabilities of Large Language Models (LLMs) by incorporating external knowledge into their processing. While LLMs have shown great promise, they can struggle with tasks requiring specific or up-to-date information, sometimes generating inaccurate outputs, known as \"hallucinations\".\n",
        "RAG aims to solve this by retrieving relevant information from external databases to supplement the LLM's internal knowledge. This process makes the LLM's outputs more accurate, credible, and grounded in factual data https://arxiv.org/abs/2312.10997.\n",
        "Here's a breakdown of how RAG typically works:\n",
        "User Interaction: A user poses a question or provides a prompt to the RAG system.\n",
        "Retrieval Phase:\n",
        "Query Processing: The user's query is transformed into a format suitable for retrieval. This may involve techniques like query expansion, rewriting, or transformation to improve its clarity and relevance for searching external data sources.\n",
        "Search and Retrieval: The processed query is used to search an external knowledge base, which could consist of text documents, databases, knowledge graphs, or even previously generated LLM content. The system retrieves the most relevant chunks of information based on semantic similarity calculations.\n",
        "Generation Phase:\n",
        "Context Integration: The retrieved information is combined with the user's original query to create a comprehensive prompt for the LLM.\n",
        "LLM Processing: The LLM processes the enriched prompt, drawing on both its internal knowledge and the retrieved external information to generate a response.\n",
        "Output Generation: The LLM generates a response to the user's query, informed by both its own knowledge and the relevant external information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b0547a",
      "metadata": {
        "papermill": {
          "duration": 0.019334,
          "end_time": "2024-11-30T20:16:16.284644",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.265310",
          "status": "completed"
        },
        "tags": [],
        "id": "47b0547a"
      },
      "source": [
        "# Model 2: knowledge base RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a682f652",
      "metadata": {
        "papermill": {
          "duration": 0.020164,
          "end_time": "2024-11-30T20:16:16.324478",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.304314",
          "status": "completed"
        },
        "tags": [],
        "id": "a682f652"
      },
      "source": [
        "Naive RAG, the earliest methodology in Retrieval-Augmented Generation, gained popularity shortly after the widespread use of ChatGPT. This approach follows a traditional, chain-like process: indexing, retrieval, and generation.\n",
        "\n",
        "Indexing:\n",
        "\n",
        "This stage involves cleaning and extracting data from various formats like PDF, HTML, and Markdown, converting it into plain text.\n",
        "To handle the context limitations of language models, the text is divided into smaller chunks.\n",
        "An embedding model is used to encode these chunks into vector representations which are stored in a vector database. This step is crucial for efficient similarity searches during retrieval.\n",
        "\n",
        "Retrieval:\n",
        "\n",
        "When a user asks a question, the same embedding model used during indexing transforms the query into a vector.\n",
        "The RAG system calculates similarity scores between the query vector and the vectors of the indexed chunks.\n",
        "The system retrieves the top K chunks with the highest similarity to the query and uses them as expanded context within the prompt.\n",
        "\n",
        "Generation:\n",
        "\n",
        "The chosen documents and the user's query are combined into a prompt, which is then fed to a large language model for response generation.\n",
        "The model may use its internal knowledge or only information from the documents to answer, depending on the task.\n",
        "For ongoing conversations, the dialogue history is integrated into the prompt, allowing for multi-turn interactions.\n",
        "\n",
        "-Limitations of Naive RAG\n",
        "\n",
        "Retrieval Challenges: The retrieval process often struggles with accurately identifying and retrieving relevant information, potentially selecting misaligned chunks or missing crucial information.\n",
        "Generation Difficulties: The model might generate content unsupported by the retrieved context (hallucination), and outputs could suffer from irrelevance, bias, or toxicity.\n",
        "\n",
        "Augmentation Hurdles: Integrating retrieved information effectively can be difficult, leading to disjointed outputs. Redundancy from similar information in multiple sources can result in repetitive responses. Further complexities arise in determining the importance of passages and ensuring stylistic consistency.\n",
        "\n",
        "Over-reliance on Retrieved Information: There is a risk that the generation models may depend too heavily on augmented information, echoing retrieved content without adding inshtful synthesis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37b89894",
      "metadata": {
        "papermill": {
          "duration": 0.019265,
          "end_time": "2024-11-30T20:16:16.404203",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.384938",
          "status": "completed"
        },
        "tags": [],
        "id": "37b89894"
      },
      "source": [
        "### Step 1: Text Processing with TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7e4e572",
      "metadata": {
        "papermill": {
          "duration": 0.019225,
          "end_time": "2024-11-30T20:16:16.442748",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.423523",
          "status": "completed"
        },
        "tags": [],
        "id": "f7e4e572"
      },
      "source": [
        "Use TF-IDF (Term Frequency-Inverse Document Frequency) to convert the extracted text into a numerical format that represents the significance of each word in the document. This will help in identifying important sections of the document to respond to queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3b01b7c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:16.483927Z",
          "iopub.status.busy": "2024-11-30T20:16:16.483302Z",
          "iopub.status.idle": "2024-11-30T20:16:16.536849Z",
          "shell.execute_reply": "2024-11-30T20:16:16.535791Z"
        },
        "papermill": {
          "duration": 0.076102,
          "end_time": "2024-11-30T20:16:16.538780",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.462678",
          "status": "completed"
        },
        "tags": [],
        "id": "c3b01b7c"
      },
      "outputs": [],
      "source": [
        "# Split paper text into sections for better vectorization (e.g., paragraphs or sentences)\n",
        "sections = papers_text[0].split('\\n')  # Split by newline or any other delimiter as per requirement\n",
        "\n",
        "# Compute TF-IDF matrix\n",
        "np.random.seed(0)\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sections)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa784356",
      "metadata": {
        "papermill": {
          "duration": 0.019808,
          "end_time": "2024-11-30T20:16:16.578872",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.559064",
          "status": "completed"
        },
        "tags": [],
        "id": "aa784356"
      },
      "source": [
        "### Step 2: Creating FAISS Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ee9bb8",
      "metadata": {
        "papermill": {
          "duration": 0.019284,
          "end_time": "2024-11-30T20:16:16.617876",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.598592",
          "status": "completed"
        },
        "tags": [],
        "id": "c1ee9bb8"
      },
      "source": [
        "Use FAISS (Facebook AI Similarity Search) for efficient similarity search. Index the TF-IDF vectors of the document segments to retrieve relevant sections for each query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed4f036",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:16.657952Z",
          "iopub.status.busy": "2024-11-30T20:16:16.657627Z",
          "iopub.status.idle": "2024-11-30T20:16:16.797061Z",
          "shell.execute_reply": "2024-11-30T20:16:16.796136Z"
        },
        "papermill": {
          "duration": 0.161877,
          "end_time": "2024-11-30T20:16:16.799110",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.637233",
          "status": "completed"
        },
        "tags": [],
        "id": "aed4f036"
      },
      "outputs": [],
      "source": [
        "# Create FAISS index\n",
        "dimension = tfidf_matrix.shape[1]  # Dimension of TF-IDF vectors\n",
        "index = faiss.IndexFlatL2(dimension)  # Using L2 (euclidean) distance\n",
        "\n",
        "# Convert the sparse matrix to dense for FAISS\n",
        "dense_vectors = tfidf_matrix.toarray()\n",
        "\n",
        "# Add vectors to the index\n",
        "index.add(dense_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdc81d18",
      "metadata": {
        "papermill": {
          "duration": 0.019424,
          "end_time": "2024-11-30T20:16:16.838876",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.819452",
          "status": "completed"
        },
        "tags": [],
        "id": "bdc81d18"
      },
      "source": [
        "### Step 3: Implementing the Query System"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc40104c",
      "metadata": {
        "papermill": {
          "duration": 0.019277,
          "end_time": "2024-11-30T20:16:16.877596",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.858319",
          "status": "completed"
        },
        "tags": [],
        "id": "fc40104c"
      },
      "source": [
        "For each user query, use FAISS to retrieve the most relevant sections of the document and pass these sections as context, along with the query, to the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "928e1eae",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:16.917835Z",
          "iopub.status.busy": "2024-11-30T20:16:16.917493Z",
          "iopub.status.idle": "2024-11-30T20:16:16.923121Z",
          "shell.execute_reply": "2024-11-30T20:16:16.922261Z"
        },
        "papermill": {
          "duration": 0.027737,
          "end_time": "2024-11-30T20:16:16.924761",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.897024",
          "status": "completed"
        },
        "tags": [],
        "id": "928e1eae"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_section(query, top_k=2):\n",
        "    # Compute query vector\n",
        "    query_vector = vectorizer.transform([query]).toarray()\n",
        "\n",
        "    # Search FAISS index\n",
        "    _, indices = index.search(query_vector, top_k)\n",
        "\n",
        "    # Retrieve relevant text sections\n",
        "    relevant_sections = [sections[i] for i in indices[0]]\n",
        "\n",
        "    return \" \".join(relevant_sections)\n",
        "\n",
        "# Function to generate a response using retrieved information and context\n",
        "def chatAI_RAG(query):\n",
        "    # Retrieve relevant sections\n",
        "    relevant_text = retrieve_relevant_section(query)\n",
        "\n",
        "    # Combine context with relevant sections and user query\n",
        "    full_prompt = f\"{context_info}\\n\\nRelevant Information:\\n{relevant_text}\\n\\nQuestion:\\n{query}\"\n",
        "    response = chat_session.send_message(full_prompt)\n",
        "\n",
        "    return response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27972e90",
      "metadata": {
        "papermill": {
          "duration": 0.020076,
          "end_time": "2024-11-30T20:16:16.964930",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.944854",
          "status": "completed"
        },
        "tags": [],
        "id": "27972e90"
      },
      "source": [
        "### Step 4: Asking Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b054fc5a",
      "metadata": {
        "papermill": {
          "duration": 0.01974,
          "end_time": "2024-11-30T20:16:17.005671",
          "exception": false,
          "start_time": "2024-11-30T20:16:16.985931",
          "status": "completed"
        },
        "tags": [],
        "id": "b054fc5a"
      },
      "source": [
        "Saves responses for each question using the chatAI_RAG function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e721c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:17.046981Z",
          "iopub.status.busy": "2024-11-30T20:16:17.046168Z",
          "iopub.status.idle": "2024-11-30T20:16:37.187500Z",
          "shell.execute_reply": "2024-11-30T20:16:37.186785Z"
        },
        "papermill": {
          "duration": 20.164215,
          "end_time": "2024-11-30T20:16:37.189487",
          "exception": false,
          "start_time": "2024-11-30T20:16:17.025272",
          "status": "completed"
        },
        "tags": [],
        "id": "53e721c5"
      },
      "outputs": [],
      "source": [
        "response_1_rag = chatAI_RAG(question_1)\n",
        "response_2_rag = chatAI_RAG(question_2)\n",
        "response_3_rag = chatAI_RAG(question_3)\n",
        "response_4_rag = chatAI_RAG(question_4)\n",
        "response_5_rag = chatAI_RAG(question_5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "231e8976",
      "metadata": {
        "papermill": {
          "duration": 0.020172,
          "end_time": "2024-11-30T20:16:37.230381",
          "exception": false,
          "start_time": "2024-11-30T20:16:37.210209",
          "status": "completed"
        },
        "tags": [],
        "id": "231e8976"
      },
      "source": [
        "### Model 3: knowledge Advanced RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13868287",
      "metadata": {
        "papermill": {
          "duration": 0.019449,
          "end_time": "2024-11-30T20:16:37.269446",
          "exception": false,
          "start_time": "2024-11-30T20:16:37.249997",
          "status": "completed"
        },
        "tags": [],
        "id": "13868287"
      },
      "source": [
        "Advanced RAG builds upon the foundation of Naive RAG, focusing on refining retrieval quality to address the limitations of the earlier approach. It employs strategies both before and after the retrieval stage to improve accuracy and relevance.\n",
        "\n",
        "-Pre-Retrieval Process.\n",
        "\n",
        "This stage focuses on optimising the indexing structure and the initial user query to ensure that the retrieval process starts with the most relevant and refined inputs.\n",
        "Indexing optimisation aims to enhance the quality of the indexed content.\n",
        "\n",
        "Optimising index structures: using techniques like hierarchical indexing or knowledge graph indexing can facilitate faster and more accurate retrieval.\n",
        "Adding metadata, enriching chunks with metadata allows for targeted filtering during retrieval.\n",
        "\n",
        "Alignment optimisation: ensuring that the indexing and retrieval processes are aligned can improve the accuracy of retrieved information.\n",
        "Mixed retrieval, combining different retrieval methods can leverage their complementary strengths.\n",
        "Query optimisation focuses on refining the user's question to be more suitable for retrieval.\n",
        "\n",
        "Query rewriting: LLMs can be used to rephrase the original query into a format that is more effective for retrieval.\n",
        "\n",
        "Query transformation: this involves techniques like using prompt engineering to generate a new query based on the original one.\n",
        "\n",
        "Query expansion: expanding the query into multiple queries or generating sub-queries can provide additional context and improve retrieval relevance.\n",
        "\n",
        "-Post-Retrieval Process\n",
        "\n",
        "After retrieving potentially relevant context, Advanced RAG implements strategies to effectively integrate this information with the query and prepare it for the language model.\n",
        "\n",
        "Reranking chunks: reordering the retrieved information to prioritise the most relevant content at the beginning of the prompt enhances the language model's focus. This is implemented in frameworks like LlamaIndex, LangChain, and Haystack.\n",
        "\n",
        "Context compression: to prevent information overload and ensure the language model focuses on essential details, post-retrieval efforts concentrate on:\n",
        "selecting the most important information from the retrieved content;\n",
        "emphasising critical sections within the retrieved documents;\n",
        "shortening the context to be processed by the language model.\n",
        "\n",
        "By addressing the limitations of Naive RAG through these pre- and post-retrieval strategies, Advanced RAG achieves significant improvements in retrieval quality, paving the way for more accurate and relevant responsesrom the language model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Text Processing with semantic embeddings"
      ],
      "metadata": {
        "id": "cXX8aIZEUQsw"
      },
      "id": "cXX8aIZEUQsw"
    },
    {
      "cell_type": "markdown",
      "id": "99fedcd6",
      "metadata": {
        "papermill": {
          "duration": 0.019741,
          "end_time": "2024-11-30T20:16:37.388817",
          "exception": false,
          "start_time": "2024-11-30T20:16:37.369076",
          "status": "completed"
        },
        "tags": [],
        "id": "99fedcd6"
      },
      "source": [
        "Load a pre-trained sentence transformer model, which is used to generate sentence embeddings.\n",
        "'all-MiniLM-L6-v2' is a specific variant of the transformer model that provides a balance between performance and computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99f255f6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:37.430254Z",
          "iopub.status.busy": "2024-11-30T20:16:37.429485Z",
          "iopub.status.idle": "2024-11-30T20:16:41.050875Z",
          "shell.execute_reply": "2024-11-30T20:16:41.050072Z"
        },
        "papermill": {
          "duration": 3.644375,
          "end_time": "2024-11-30T20:16:41.052930",
          "exception": false,
          "start_time": "2024-11-30T20:16:37.408555",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "cb91f1ed4dd94ee3833096c054401199",
            "d0ded41b3f5d491599d37185307a33ac",
            "258ad788870841cfb6615b283fe31124",
            "595cafb0e142465d950531b1c73d87aa",
            "a2738ba4be3e4f939d3998a245e11eb4",
            "72bcdbb013e444c59cae0503ddda53bd",
            "300d071f5c7349b2923cc1f74c987231",
            "d6f5018929af44d0843b29e01ee57d46",
            "a68409a2eab04929803ebc08765578c6",
            "feb6dd6431a14baebaeb2f64db2e3d52",
            "16065f5cbb6c4cd1b4e39718529a7adb"
          ]
        },
        "id": "99f255f6",
        "outputId": "df4fe2f6-697d-4feb-9e01-f27765a8656e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb91f1ed4dd94ee3833096c054401199",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0ded41b3f5d491599d37185307a33ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "258ad788870841cfb6615b283fe31124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "595cafb0e142465d950531b1c73d87aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2738ba4be3e4f939d3998a245e11eb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72bcdbb013e444c59cae0503ddda53bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "300d071f5c7349b2923cc1f74c987231",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6f5018929af44d0843b29e01ee57d46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a68409a2eab04929803ebc08765578c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feb6dd6431a14baebaeb2f64db2e3d52",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16065f5cbb6c4cd1b4e39718529a7adb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load a sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0891f090",
      "metadata": {
        "papermill": {
          "duration": 0.020875,
          "end_time": "2024-11-30T20:16:41.095485",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.074610",
          "status": "completed"
        },
        "tags": [],
        "id": "0891f090"
      },
      "source": [
        "Define a function that re-ranks documents based on a query with semantic similarity measures from a transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f933cf8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:41.139098Z",
          "iopub.status.busy": "2024-11-30T20:16:41.138386Z",
          "iopub.status.idle": "2024-11-30T20:16:41.144075Z",
          "shell.execute_reply": "2024-11-30T20:16:41.143359Z"
        },
        "papermill": {
          "duration": 0.02919,
          "end_time": "2024-11-30T20:16:41.145570",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.116380",
          "status": "completed"
        },
        "tags": [],
        "id": "8f933cf8"
      },
      "outputs": [],
      "source": [
        "def re_rank_documents(query, documents):\n",
        "    # Encode the query and documents using the transformer model\n",
        "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
        "    doc_embeddings = model.encode(documents, convert_to_tensor=True)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    cosine_scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "    # Sort documents based on descending cosine similarity scores\n",
        "    scores_and_docs = sorted(zip(cosine_scores.tolist(), documents), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Retrieve top ranked documents and their scores\n",
        "    top_ranked_docs = [doc for _, doc in scores_and_docs]\n",
        "    return top_ranked_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e07eff9d",
      "metadata": {
        "papermill": {
          "duration": 0.022659,
          "end_time": "2024-11-30T20:16:41.188859",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.166200",
          "status": "completed"
        },
        "tags": [],
        "id": "e07eff9d"
      },
      "source": [
        "### Step 2: Text Processing with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33e9608",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:41.232190Z",
          "iopub.status.busy": "2024-11-30T20:16:41.231574Z",
          "iopub.status.idle": "2024-11-30T20:16:41.345741Z",
          "shell.execute_reply": "2024-11-30T20:16:41.345019Z"
        },
        "papermill": {
          "duration": 0.138077,
          "end_time": "2024-11-30T20:16:41.347613",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.209536",
          "status": "completed"
        },
        "tags": [],
        "id": "e33e9608"
      },
      "outputs": [],
      "source": [
        "# Pre-process and index documents for initial retrieval\n",
        "# Split a text (likely a scientific paper or a large document) into sections using newline characters as delimiters.\n",
        "sections = papers_text[0].split('\\n')\n",
        "# Create a TF-IDF vectorizer and use it to transform the sections into a TF-IDF matrix representation.\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(sections)\n",
        "# Convert the sparse TF-IDF matrix to a dense representation (array format).\n",
        "dense_vectors = tfidf_matrix.toarray()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e226057c",
      "metadata": {
        "papermill": {
          "duration": 0.020785,
          "end_time": "2024-11-30T20:16:41.390353",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.369568",
          "status": "completed"
        },
        "tags": [],
        "id": "e226057c"
      },
      "source": [
        "### Step 3: Creating FAISS Index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4e41dc",
      "metadata": {
        "papermill": {
          "duration": 0.020951,
          "end_time": "2024-11-30T20:16:41.431810",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.410859",
          "status": "completed"
        },
        "tags": [],
        "id": "ce4e41dc"
      },
      "source": [
        "Create a FAISS index for fast nearest-neighbor search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "758b68df",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:41.475202Z",
          "iopub.status.busy": "2024-11-30T20:16:41.474401Z",
          "iopub.status.idle": "2024-11-30T20:16:41.545695Z",
          "shell.execute_reply": "2024-11-30T20:16:41.545005Z"
        },
        "papermill": {
          "duration": 0.094904,
          "end_time": "2024-11-30T20:16:41.547663",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.452759",
          "status": "completed"
        },
        "tags": [],
        "id": "758b68df"
      },
      "outputs": [],
      "source": [
        "# Create FAISS index\n",
        "dimension = tfidf_matrix.shape[1] # The number of features (or terms) in the TF-IDF matrix.\n",
        "index = faiss.IndexFlatL2(dimension) # A FAISS index using the L2 (Euclidean) distance metric, often used for dense vector similarity.\n",
        "index.add(dense_vectors) # Add the dense TF-IDF vectors to the index.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b87e21c",
      "metadata": {
        "papermill": {
          "duration": 0.020473,
          "end_time": "2024-11-30T20:16:41.589560",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.569087",
          "status": "completed"
        },
        "tags": [],
        "id": "7b87e21c"
      },
      "source": [
        "### Step 4: Implementing the Query System with re-ranking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703f2713",
      "metadata": {
        "papermill": {
          "duration": 0.020324,
          "end_time": "2024-11-30T20:16:41.630369",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.610045",
          "status": "completed"
        },
        "tags": [],
        "id": "703f2713"
      },
      "source": [
        "Define a function to retrieve the most relevant sections from the text based on a query.\n",
        "Define a function to generate a conversational response based on a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a51dc6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:41.672889Z",
          "iopub.status.busy": "2024-11-30T20:16:41.672527Z",
          "iopub.status.idle": "2024-11-30T20:16:41.678201Z",
          "shell.execute_reply": "2024-11-30T20:16:41.677260Z"
        },
        "papermill": {
          "duration": 0.029171,
          "end_time": "2024-11-30T20:16:41.679985",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.650814",
          "status": "completed"
        },
        "tags": [],
        "id": "d3a51dc6"
      },
      "outputs": [],
      "source": [
        "def retrieve_relevant_section(query, top_k=10):\n",
        "    # Compute query vector\n",
        "    query_vector = vectorizer.transform([query]).toarray()\n",
        "\n",
        "    # Search FAISS index for initial retrieval\n",
        "    _, indices = index.search(query_vector, top_k)\n",
        "    initial_relevant_sections = [sections[i] for i in indices[0]]\n",
        "\n",
        "    # Re-rank the initially retrieved sections\n",
        "    top_ranked_docs = re_rank_documents(query, initial_relevant_sections)\n",
        "\n",
        "    # Select a subset for the final response, if desired\n",
        "    return \" \".join(top_ranked_docs[:2])  # Take top 2 after re-ranking\n",
        "\n",
        "def chatAI_ARAG(query):\n",
        "    relevant_text = retrieve_relevant_section(query)\n",
        "    full_prompt = f\"{context_info}\\n\\nRelevant Information:\\n{relevant_text}\\n\\nQuestion:\\n{query}\"\n",
        "    response = chat_session.send_message(full_prompt)\n",
        "    return response.text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a9291cd",
      "metadata": {
        "papermill": {
          "duration": 0.020366,
          "end_time": "2024-11-30T20:16:41.724426",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.704060",
          "status": "completed"
        },
        "tags": [],
        "id": "0a9291cd"
      },
      "source": [
        "### Step 5: Asking Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149630b2",
      "metadata": {
        "papermill": {
          "duration": 0.020544,
          "end_time": "2024-11-30T20:16:41.767900",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.747356",
          "status": "completed"
        },
        "tags": [],
        "id": "149630b2"
      },
      "source": [
        "Call the chatAI_ARAG function for multiple queries and store the responses in separate variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f125a3f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:41.814803Z",
          "iopub.status.busy": "2024-11-30T20:16:41.814431Z",
          "iopub.status.idle": "2024-11-30T20:16:58.364488Z",
          "shell.execute_reply": "2024-11-30T20:16:58.363615Z"
        },
        "papermill": {
          "duration": 16.576878,
          "end_time": "2024-11-30T20:16:58.366681",
          "exception": false,
          "start_time": "2024-11-30T20:16:41.789803",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "937528bd5592413bb6d92d8665c2f124",
            "5a698161a0a84abc870496c5efc23427",
            "3668b3858b5042b0bdc1ad761994b6e6",
            "1c2de73769884587bac686df4f5dd3ae",
            "6792559a8a014046bdbafa31878acace",
            "1b819fc18b2f4b9b8644ed0a410eb2c4",
            "b41cf79e807f46c884cf4bbef6085d05",
            "6025ddf9ffd249b288cc3b3181fa3859",
            "a2270feeab79488c83ef73dbc103c0ac",
            "e0378bb04c124e07ad172c8517211059"
          ]
        },
        "id": "2f125a3f",
        "outputId": "5340d1ea-0882-4a47-cc1f-e6781807389d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "937528bd5592413bb6d92d8665c2f124",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a698161a0a84abc870496c5efc23427",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3668b3858b5042b0bdc1ad761994b6e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c2de73769884587bac686df4f5dd3ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6792559a8a014046bdbafa31878acace",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b819fc18b2f4b9b8644ed0a410eb2c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b41cf79e807f46c884cf4bbef6085d05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6025ddf9ffd249b288cc3b3181fa3859",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2270feeab79488c83ef73dbc103c0ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0378bb04c124e07ad172c8517211059",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "response_1_arag = chatAI_ARAG(question_1)\n",
        "response_2_arag = chatAI_ARAG(question_2)\n",
        "response_3_arag = chatAI_ARAG(question_3)\n",
        "response_4_arag = chatAI_ARAG(question_4)\n",
        "response_5_arag = chatAI_ARAG(question_5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956b322a",
      "metadata": {
        "papermill": {
          "duration": 0.023748,
          "end_time": "2024-11-30T20:16:58.414981",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.391233",
          "status": "completed"
        },
        "tags": [],
        "id": "956b322a"
      },
      "source": [
        "### Right answers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5281f67a",
      "metadata": {
        "papermill": {
          "duration": 0.022365,
          "end_time": "2024-11-30T20:16:58.459190",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.436825",
          "status": "completed"
        },
        "tags": [],
        "id": "5281f67a"
      },
      "source": [
        "List of the actual answers retrieved from the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64b69a39",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:58.504169Z",
          "iopub.status.busy": "2024-11-30T20:16:58.503558Z",
          "iopub.status.idle": "2024-11-30T20:16:58.508127Z",
          "shell.execute_reply": "2024-11-30T20:16:58.507240Z"
        },
        "papermill": {
          "duration": 0.028874,
          "end_time": "2024-11-30T20:16:58.509837",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.480963",
          "status": "completed"
        },
        "tags": [],
        "id": "64b69a39"
      },
      "outputs": [],
      "source": [
        "right_answer_1 = \"\"\"\n",
        "Encoder-only models:\n",
        "These models are advantageous for text classification tasks such as sentiment analysis. An example of an LLM utilizing an encoder-only model is BERT.\n",
        "\n",
        "Decoder-only or autoregressive models:\n",
        "These models are suitable for text generation tasks, akin to the predictive text functionality in a smartphone chat application. For instance, as you input text, the AI predicts the subsequent word or phrase. An example of this model is GPT-3.\n",
        "\n",
        "Encoder-decoder models:\n",
        "These models facilitate generative AI tasks such as language translation and summarization. Notable LLMs employing this methodology include Facebooks BART and Googles T5.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc69d9a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:58.554359Z",
          "iopub.status.busy": "2024-11-30T20:16:58.553844Z",
          "iopub.status.idle": "2024-11-30T20:16:58.557578Z",
          "shell.execute_reply": "2024-11-30T20:16:58.556784Z"
        },
        "papermill": {
          "duration": 0.027618,
          "end_time": "2024-11-30T20:16:58.559337",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.531719",
          "status": "completed"
        },
        "tags": [],
        "id": "6cc69d9a"
      },
      "outputs": [],
      "source": [
        "right_answer_2 = \"\"\"\n",
        "MERGIS, was proposed by (Nimalsiri et al. 2023). It uses image segmentation and a modern\n",
        "transformer-based encoder-decoder model to enhance the accuracy of automated report generation.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35d846b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:58.603631Z",
          "iopub.status.busy": "2024-11-30T20:16:58.603372Z",
          "iopub.status.idle": "2024-11-30T20:16:58.606921Z",
          "shell.execute_reply": "2024-11-30T20:16:58.606273Z"
        },
        "papermill": {
          "duration": 0.027577,
          "end_time": "2024-11-30T20:16:58.608410",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.580833",
          "status": "completed"
        },
        "tags": [],
        "id": "c35d846b"
      },
      "outputs": [],
      "source": [
        "right_answer_3 = \"\"\"\n",
        "(zbey et al. 2023) introduced an innovative technique named Adaptive Diffusion\n",
        "Priors (AdaDiff) for the reconstruction of MRI. This approach involves a series of diffusion processes\n",
        "that enhance the authenticity of the generated images. AdaDiff dynamically adjusts its priors during\n",
        "the inference stage to align more closely with the distribution of the test data.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961f5456",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:58.652380Z",
          "iopub.status.busy": "2024-11-30T20:16:58.652107Z",
          "iopub.status.idle": "2024-11-30T20:16:58.656110Z",
          "shell.execute_reply": "2024-11-30T20:16:58.655428Z"
        },
        "papermill": {
          "duration": 0.027625,
          "end_time": "2024-11-30T20:16:58.657671",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.630046",
          "status": "completed"
        },
        "tags": [],
        "id": "961f5456"
      },
      "outputs": [],
      "source": [
        "right_answer_4 =\"\"\"\n",
        "The forward diffusion process is depicted as a Markov Chain, characterized by the inclusion of\n",
        "Gaussian noise in a series of stages, culminating in generating noisy samples. The uncorrupted\n",
        "or original data distribution is represented as (0). With a data sample 0 drawn from this\n",
        "distribution, (0), a forward noising operation, denoted as , is employed.\n",
        "This operation introduces Gaussian noise iteratively at various time points, represented by ,\n",
        "resulting in a series of latent states 1 through . The process can be mathematically defined as\n",
        "follows:\n",
        "( | 1)=(:1 .1,. ),{1,,}\n",
        " denotes the number of diffusion steps, while 1,, , each within the interval of [0, 1), signify\n",
        "the variance schedule spread throughout the diffusion steps.\n",
        "The identity matrix is symbolized by , and (; ,), which characterizes the normal distribution\n",
        "possessing a mean of  and a covariance of .\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a374b90",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:58.701613Z",
          "iopub.status.busy": "2024-11-30T20:16:58.701338Z",
          "iopub.status.idle": "2024-11-30T20:16:58.705012Z",
          "shell.execute_reply": "2024-11-30T20:16:58.704207Z"
        },
        "papermill": {
          "duration": 0.027534,
          "end_time": "2024-11-30T20:16:58.706686",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.679152",
          "status": "completed"
        },
        "tags": [],
        "id": "1a374b90"
      },
      "outputs": [],
      "source": [
        "right_answer_5 =\"\"\"\n",
        "ProteinBERT was developed by (Brandes et al. 2022). It's a specialized deep language model for protein\n",
        "sequences that amalgamates local and global representations for comprehensive end-to-end processing.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5daa29c",
      "metadata": {
        "papermill": {
          "duration": 0.021085,
          "end_time": "2024-11-30T20:16:58.749440",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.728355",
          "status": "completed"
        },
        "tags": [],
        "id": "a5daa29c"
      },
      "source": [
        "### Responses and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6afe2064",
      "metadata": {
        "papermill": {
          "duration": 0.021008,
          "end_time": "2024-11-30T20:16:58.791789",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.770781",
          "status": "completed"
        },
        "tags": [],
        "id": "6afe2064"
      },
      "source": [
        "LLM evaluation metrics are used to assess the quality of text generated by Large Language Models (LLMs). They provide a quantifiable measure of how well the LLM is performing on a specific task. While general-purpose metrics exist, it's crucial to select metrics tailored to the specific use case of the LLM application.\n",
        "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a statistical metric commonly used for evaluating text summaries generated by NLP models. It calculates the overlap of n-grams (sequences of words) between the generated summary and a reference summary. However, statistical methods like ROUGE are considered less effective for evaluating complex LLM outputs because they don't fully capture the semantic nuances and reasoning involved.\n",
        "BERTScore, a model-based metric, offers a more semantically aware approach. It leverages pre-trained language models like BERT to compute the similarity between the contextual embeddings of words in the generated text and the reference text. However, BERTScore can be susceptible to biases present in the pre-trained models and may struggle with long, complex texts\n",
        "https://www.confident-ai.com/blog/llm-evaluation-metrics-everything-you-need-for-llm-evaluation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d7bca89",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:58.836280Z",
          "iopub.status.busy": "2024-11-30T20:16:58.835600Z",
          "iopub.status.idle": "2024-11-30T20:16:58.841625Z",
          "shell.execute_reply": "2024-11-30T20:16:58.840783Z"
        },
        "papermill": {
          "duration": 0.02995,
          "end_time": "2024-11-30T20:16:58.843266",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.813316",
          "status": "completed"
        },
        "tags": [],
        "id": "4d7bca89",
        "outputId": "ba3da17c-0776-4ff9-ab6b-c791c420fb97"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The review mentions three types of encoder-decoder transformer architectures:\n",
              "\n",
              "1.  Encoder-only models (e.g., BERT) suitable for text classification tasks.\n",
              "2.  Decoder-only or autoregressive models (e.g., GPT-3) suitable for text generation tasks.\n",
              "3.  Encoder-decoder models (e.g., Facebook's BART and Google's T5) suitable for tasks like language translation and summarization.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_1_base))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7650253a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:16:58.888474Z",
          "iopub.status.busy": "2024-11-30T20:16:58.887834Z",
          "iopub.status.idle": "2024-11-30T20:17:04.036197Z",
          "shell.execute_reply": "2024-11-30T20:17:04.035123Z"
        },
        "papermill": {
          "duration": 5.172872,
          "end_time": "2024-11-30T20:17:04.037983",
          "exception": false,
          "start_time": "2024-11-30T20:16:58.865111",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "cf870b7f6f7746beb5efba5d1855355e",
            "3c6803c9719748338681531daadf9521",
            "70341dbf9db94004af44a2c76d5d1f81",
            "e8319bc8404b4b39be6a031271cc201d",
            "1b727d60f3454602b605bf4ef9f94fc2"
          ]
        },
        "id": "7650253a",
        "outputId": "6b9f2f14-983b-4efe-ebb0-e2410fa1315a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.6724, recall=0.3861, fmeasure=0.4906\n",
            "rouge2: precision=0.4561, recall=0.2600, fmeasure=0.3312\n",
            "rougeL: precision=0.4828, recall=0.2772, fmeasure=0.3522\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf870b7f6f7746beb5efba5d1855355e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c6803c9719748338681531daadf9521",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70341dbf9db94004af44a2c76d5d1f81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8319bc8404b4b39be6a031271cc201d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b727d60f3454602b605bf4ef9f94fc2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BERTScore: Precision=0.7877, Recall=0.7473, F1=0.7670\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_1, response_1_base)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_1, response_1_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb498e7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:04.085631Z",
          "iopub.status.busy": "2024-11-30T20:17:04.084913Z",
          "iopub.status.idle": "2024-11-30T20:17:04.090345Z",
          "shell.execute_reply": "2024-11-30T20:17:04.089508Z"
        },
        "papermill": {
          "duration": 0.03048,
          "end_time": "2024-11-30T20:17:04.091873",
          "exception": false,
          "start_time": "2024-11-30T20:17:04.061393",
          "status": "completed"
        },
        "tags": [],
        "id": "acb498e7",
        "outputId": "fb937ab2-b62f-434a-857b-e44f3c31967e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The review discusses three types of encoder-decoder transformer architectures: encoder-only models (e.g., BERT), decoder-only or autoregressive models (e.g., GPT-3), and encoder-decoder models (e.g., Facebook's BART and Google's T5).\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_1_rag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa59e33a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:04.138149Z",
          "iopub.status.busy": "2024-11-30T20:17:04.137865Z",
          "iopub.status.idle": "2024-11-30T20:17:04.901231Z",
          "shell.execute_reply": "2024-11-30T20:17:04.900329Z"
        },
        "papermill": {
          "duration": 0.788546,
          "end_time": "2024-11-30T20:17:04.903187",
          "exception": false,
          "start_time": "2024-11-30T20:17:04.114641",
          "status": "completed"
        },
        "tags": [],
        "id": "fa59e33a",
        "outputId": "811de121-73f4-444b-e1f6-4a6eef82da2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.6579, recall=0.2475, fmeasure=0.3597\n",
            "rouge2: precision=0.4324, recall=0.1600, fmeasure=0.2336\n",
            "rougeL: precision=0.5789, recall=0.2178, fmeasure=0.3165\n",
            "BERTScore: Precision=0.7573, Recall=0.6871, F1=0.7205\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_1, response_1_rag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_1, response_1_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6dc53fc",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:04.951881Z",
          "iopub.status.busy": "2024-11-30T20:17:04.951280Z",
          "iopub.status.idle": "2024-11-30T20:17:04.956767Z",
          "shell.execute_reply": "2024-11-30T20:17:04.955949Z"
        },
        "papermill": {
          "duration": 0.030757,
          "end_time": "2024-11-30T20:17:04.958361",
          "exception": false,
          "start_time": "2024-11-30T20:17:04.927604",
          "status": "completed"
        },
        "tags": [],
        "id": "c6dc53fc",
        "outputId": "21fbd397-7cab-40a5-96a0-9cacc1c6e168"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The review mentions three types of encoder-decoder transformer architectures:  encoder-only models (like BERT), decoder-only or autoregressive models (like GPT-3), and encoder-decoder models (like BART and T5).\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_1_arag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25b843e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:05.004815Z",
          "iopub.status.busy": "2024-11-30T20:17:05.004507Z",
          "iopub.status.idle": "2024-11-30T20:17:05.705599Z",
          "shell.execute_reply": "2024-11-30T20:17:05.704621Z"
        },
        "papermill": {
          "duration": 0.726148,
          "end_time": "2024-11-30T20:17:05.707455",
          "exception": false,
          "start_time": "2024-11-30T20:17:04.981307",
          "status": "completed"
        },
        "tags": [],
        "id": "d25b843e",
        "outputId": "fe619895-1f12-4b5f-da50-ec30df7a3748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.6774, recall=0.2079, fmeasure=0.3182\n",
            "rouge2: precision=0.3667, recall=0.1100, fmeasure=0.1692\n",
            "rougeL: precision=0.5806, recall=0.1782, fmeasure=0.2727\n",
            "BERTScore: Precision=0.7824, Recall=0.6746, F1=0.7245\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_1, response_1_arag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_1, response_1_arag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e2a764",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:05.755445Z",
          "iopub.status.busy": "2024-11-30T20:17:05.754530Z",
          "iopub.status.idle": "2024-11-30T20:17:05.760346Z",
          "shell.execute_reply": "2024-11-30T20:17:05.759584Z"
        },
        "papermill": {
          "duration": 0.03103,
          "end_time": "2024-11-30T20:17:05.761923",
          "exception": false,
          "start_time": "2024-11-30T20:17:05.730893",
          "status": "completed"
        },
        "tags": [],
        "id": "a3e2a764",
        "outputId": "2909e8b6-759e-402b-ffb1-d508a0f8cec2"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "MERGIS was developed by Nimalsiri et al.  It is a transformer-based encoder-decoder model that uses image segmentation to enhance the accuracy of automated report generation.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_2_base))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d86463f3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:05.809990Z",
          "iopub.status.busy": "2024-11-30T20:17:05.809666Z",
          "iopub.status.idle": "2024-11-30T20:17:06.506627Z",
          "shell.execute_reply": "2024-11-30T20:17:06.505451Z"
        },
        "papermill": {
          "duration": 0.724086,
          "end_time": "2024-11-30T20:17:06.508661",
          "exception": false,
          "start_time": "2024-11-30T20:17:05.784575",
          "status": "completed"
        },
        "tags": [],
        "id": "d86463f3",
        "outputId": "d67f58be-c276-422c-fd85-2050abab4df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.8889, recall=0.8571, fmeasure=0.8727\n",
            "rouge2: precision=0.6538, recall=0.6296, fmeasure=0.6415\n",
            "rougeL: precision=0.7778, recall=0.7500, fmeasure=0.7636\n",
            "BERTScore: Precision=0.9364, Recall=0.9032, F1=0.9195\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_2, response_2_base)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_2, response_2_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "425b9575",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:06.558226Z",
          "iopub.status.busy": "2024-11-30T20:17:06.557901Z",
          "iopub.status.idle": "2024-11-30T20:17:06.563563Z",
          "shell.execute_reply": "2024-11-30T20:17:06.562744Z"
        },
        "papermill": {
          "duration": 0.031242,
          "end_time": "2024-11-30T20:17:06.565175",
          "exception": false,
          "start_time": "2024-11-30T20:17:06.533933",
          "status": "completed"
        },
        "tags": [],
        "id": "425b9575",
        "outputId": "9d65c900-60cd-474d-cf7e-517e92770603"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Nimalsiri et al. developed MERGIS.  It's a transformer-based encoder-decoder model that uses image segmentation to improve the accuracy of automated report generation.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_2_rag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51e1fc22",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:06.613093Z",
          "iopub.status.busy": "2024-11-30T20:17:06.612309Z",
          "iopub.status.idle": "2024-11-30T20:17:07.310750Z",
          "shell.execute_reply": "2024-11-30T20:17:07.309752Z"
        },
        "papermill": {
          "duration": 0.724101,
          "end_time": "2024-11-30T20:17:07.312581",
          "exception": false,
          "start_time": "2024-11-30T20:17:06.588480",
          "status": "completed"
        },
        "tags": [],
        "id": "51e1fc22",
        "outputId": "da9627e2-db34-4525-8534-864e8a8068ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.8400, recall=0.7500, fmeasure=0.7925\n",
            "rouge2: precision=0.5417, recall=0.4815, fmeasure=0.5098\n",
            "rougeL: precision=0.6800, recall=0.6071, fmeasure=0.6415\n",
            "BERTScore: Precision=0.9198, Recall=0.8871, F1=0.9032\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_2, response_2_rag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_2, response_2_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e024c270",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:07.361398Z",
          "iopub.status.busy": "2024-11-30T20:17:07.361095Z",
          "iopub.status.idle": "2024-11-30T20:17:07.366351Z",
          "shell.execute_reply": "2024-11-30T20:17:07.365610Z"
        },
        "papermill": {
          "duration": 0.031298,
          "end_time": "2024-11-30T20:17:07.367885",
          "exception": false,
          "start_time": "2024-11-30T20:17:07.336587",
          "status": "completed"
        },
        "tags": [],
        "id": "e024c270",
        "outputId": "fdfa824e-c3b6-498d-d93d-b85fcb47403e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Nimalsiri et al. developed MERGIS.  It is a transformer-based encoder-decoder model that uses image segmentation to improve the accuracy of automated report generation.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_2_arag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d8b81f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:07.414687Z",
          "iopub.status.busy": "2024-11-30T20:17:07.414419Z",
          "iopub.status.idle": "2024-11-30T20:17:08.096356Z",
          "shell.execute_reply": "2024-11-30T20:17:08.095300Z"
        },
        "papermill": {
          "duration": 0.707522,
          "end_time": "2024-11-30T20:17:08.098351",
          "exception": false,
          "start_time": "2024-11-30T20:17:07.390829",
          "status": "completed"
        },
        "tags": [],
        "id": "f7d8b81f",
        "outputId": "f670d898-c1af-4fab-e014-80cf95b08cb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.8400, recall=0.7500, fmeasure=0.7925\n",
            "rouge2: precision=0.5417, recall=0.4815, fmeasure=0.5098\n",
            "rougeL: precision=0.6800, recall=0.6071, fmeasure=0.6415\n",
            "BERTScore: Precision=0.9269, Recall=0.8885, F1=0.9073\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_2, response_2_arag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_2, response_2_arag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48f53da",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:08.148972Z",
          "iopub.status.busy": "2024-11-30T20:17:08.148181Z",
          "iopub.status.idle": "2024-11-30T20:17:08.153829Z",
          "shell.execute_reply": "2024-11-30T20:17:08.153077Z"
        },
        "papermill": {
          "duration": 0.031915,
          "end_time": "2024-11-30T20:17:08.155369",
          "exception": false,
          "start_time": "2024-11-30T20:17:08.123454",
          "status": "completed"
        },
        "tags": [],
        "id": "b48f53da",
        "outputId": "e51d7106-82ee-41c4-a818-d5c18b1965b5"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "AdaDiff was developed by zbey et al.  It is an Adaptive Diffusion Priors method for MRI reconstruction that dynamically adjusts its priors during inference to better match the test data distribution, leading to improved reconstruction quality and speed.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_3_base))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51bc9075",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:08.202967Z",
          "iopub.status.busy": "2024-11-30T20:17:08.202719Z",
          "iopub.status.idle": "2024-11-30T20:17:08.906625Z",
          "shell.execute_reply": "2024-11-30T20:17:08.905559Z"
        },
        "papermill": {
          "duration": 0.729866,
          "end_time": "2024-11-30T20:17:08.908715",
          "exception": false,
          "start_time": "2024-11-30T20:17:08.178849",
          "status": "completed"
        },
        "tags": [],
        "id": "51bc9075",
        "outputId": "41bdec9a-720a-494c-f913-c24a55243cde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.6053, recall=0.4259, fmeasure=0.5000\n",
            "rouge2: precision=0.2703, recall=0.1887, fmeasure=0.2222\n",
            "rougeL: precision=0.5263, recall=0.3704, fmeasure=0.4348\n",
            "BERTScore: Precision=0.8303, Recall=0.7937, F1=0.8116\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_3, response_3_base)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_3, response_3_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fadbb196",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:08.963813Z",
          "iopub.status.busy": "2024-11-30T20:17:08.963126Z",
          "iopub.status.idle": "2024-11-30T20:17:08.969131Z",
          "shell.execute_reply": "2024-11-30T20:17:08.968335Z"
        },
        "papermill": {
          "duration": 0.03549,
          "end_time": "2024-11-30T20:17:08.970849",
          "exception": false,
          "start_time": "2024-11-30T20:17:08.935359",
          "status": "completed"
        },
        "tags": [],
        "id": "fadbb196",
        "outputId": "291bf4e1-8a43-401a-a811-f9f28b3c0fb0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "zbey et al. developed AdaDiff.  It's an Adaptive Diffusion Priors method for MRI reconstruction that dynamically adjusts its priors during inference to better match the test data distribution, resulting in superior reconstruction quality and speed.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_3_rag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ed9c02",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:09.020529Z",
          "iopub.status.busy": "2024-11-30T20:17:09.019745Z",
          "iopub.status.idle": "2024-11-30T20:17:09.722100Z",
          "shell.execute_reply": "2024-11-30T20:17:09.721156Z"
        },
        "papermill": {
          "duration": 0.728714,
          "end_time": "2024-11-30T20:17:09.723964",
          "exception": false,
          "start_time": "2024-11-30T20:17:08.995250",
          "status": "completed"
        },
        "tags": [],
        "id": "30ed9c02",
        "outputId": "d66ffd99-d0ff-4eb3-8b0f-64cb5bd2e7dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.6389, recall=0.4259, fmeasure=0.5111\n",
            "rouge2: precision=0.2857, recall=0.1887, fmeasure=0.2273\n",
            "rougeL: precision=0.5556, recall=0.3704, fmeasure=0.4444\n",
            "BERTScore: Precision=0.8316, Recall=0.7973, F1=0.8141\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_3, response_3_rag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_3, response_3_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "770ee0b9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:09.776295Z",
          "iopub.status.busy": "2024-11-30T20:17:09.775512Z",
          "iopub.status.idle": "2024-11-30T20:17:09.780749Z",
          "shell.execute_reply": "2024-11-30T20:17:09.780016Z"
        },
        "papermill": {
          "duration": 0.033847,
          "end_time": "2024-11-30T20:17:09.782445",
          "exception": false,
          "start_time": "2024-11-30T20:17:09.748598",
          "status": "completed"
        },
        "tags": [],
        "id": "770ee0b9",
        "outputId": "e3b27f12-02a9-417b-b812-59bb7f0fa869"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "zbey et al. developed AdaDiff. It is an Adaptive Diffusion Priors method for MRI reconstruction that dynamically adjusts its priors during inference to better match the test data distribution, leading to superior reconstruction quality and speed.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_3_arag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec136ec4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:09.849094Z",
          "iopub.status.busy": "2024-11-30T20:17:09.848508Z",
          "iopub.status.idle": "2024-11-30T20:17:10.565329Z",
          "shell.execute_reply": "2024-11-30T20:17:10.564475Z"
        },
        "papermill": {
          "duration": 0.749401,
          "end_time": "2024-11-30T20:17:10.567234",
          "exception": false,
          "start_time": "2024-11-30T20:17:09.817833",
          "status": "completed"
        },
        "tags": [],
        "id": "ec136ec4",
        "outputId": "2733b80b-e471-4dec-f59a-b453f1f5c5ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.6389, recall=0.4259, fmeasure=0.5111\n",
            "rouge2: precision=0.2857, recall=0.1887, fmeasure=0.2273\n",
            "rougeL: precision=0.5556, recall=0.3704, fmeasure=0.4444\n",
            "BERTScore: Precision=0.8348, Recall=0.7984, F1=0.8162\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_3, response_3_arag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_3, response_3_arag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9eb99b0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:10.618005Z",
          "iopub.status.busy": "2024-11-30T20:17:10.617240Z",
          "iopub.status.idle": "2024-11-30T20:17:10.622737Z",
          "shell.execute_reply": "2024-11-30T20:17:10.621929Z"
        },
        "papermill": {
          "duration": 0.032144,
          "end_time": "2024-11-30T20:17:10.624357",
          "exception": false,
          "start_time": "2024-11-30T20:17:10.592213",
          "status": "completed"
        },
        "tags": [],
        "id": "d9eb99b0",
        "outputId": "bbb3ba6d-aa3a-4382-ad46-7c4a9037c861"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Equation (1),  ( | 1)=(:1 .1,. ),{1,,}, represents the forward diffusion process in Denoising Diffusion Probabilistic Models (DDPMs).  This process is a Markov chain where Gaussian noise is iteratively added to the input data (x<sub>t-1</sub>) at each time step (t).  The amount of noise added is controlled by the variance schedule (<sub>t</sub>), resulting in a series of increasingly noisy latent states (x<sub>1</sub> through x<sub>T</sub>).  The equation shows that each noisy state x<sub>t</sub> is drawn from a normal distribution () with a mean dependent on the previous state (scaled by 1 ) and a variance (<sub>t</sub>I), where I is the identity matrix.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_4_base))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f00a12",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:10.673811Z",
          "iopub.status.busy": "2024-11-30T20:17:10.673244Z",
          "iopub.status.idle": "2024-11-30T20:17:11.408450Z",
          "shell.execute_reply": "2024-11-30T20:17:11.407355Z"
        },
        "papermill": {
          "duration": 0.76189,
          "end_time": "2024-11-30T20:17:11.410287",
          "exception": false,
          "start_time": "2024-11-30T20:17:10.648397",
          "status": "completed"
        },
        "tags": [],
        "id": "c5f00a12",
        "outputId": "d914143f-c9cb-4d67-aace-c5da30124871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.5517, recall=0.4848, fmeasure=0.5161\n",
            "rouge2: precision=0.2174, recall=0.1908, fmeasure=0.2033\n",
            "rougeL: precision=0.2845, recall=0.2500, fmeasure=0.2661\n",
            "BERTScore: Precision=0.7437, Recall=0.7865, F1=0.7645\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_4, response_4_base)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_4, response_4_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d102270",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:11.460314Z",
          "iopub.status.busy": "2024-11-30T20:17:11.460038Z",
          "iopub.status.idle": "2024-11-30T20:17:11.465548Z",
          "shell.execute_reply": "2024-11-30T20:17:11.464864Z"
        },
        "papermill": {
          "duration": 0.032083,
          "end_time": "2024-11-30T20:17:11.467091",
          "exception": false,
          "start_time": "2024-11-30T20:17:11.435008",
          "status": "completed"
        },
        "tags": [],
        "id": "4d102270",
        "outputId": "1909bdaa-e410-47a6-bbe2-fad5fa7d8685"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Equation (1), ( | 1)=(:1 .1,. ),{1,,}, represents the forward diffusion process in Denoising Diffusion Probabilistic Models (DDPMs).  This is a Markov chain where Gaussian noise is iteratively added to the input data at each time step. The amount of noise added is controlled by the variance schedule (t), resulting in a series of increasingly noisy latent states.  The equation shows that each noisy state x<sub>t</sub> is drawn from a normal distribution () with a mean dependent on the previous state (scaled by 1 ) and a variance (<sub>t</sub>I), where I is the identity matrix.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_4_rag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5599230a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:11.516627Z",
          "iopub.status.busy": "2024-11-30T20:17:11.515847Z",
          "iopub.status.idle": "2024-11-30T20:17:12.225025Z",
          "shell.execute_reply": "2024-11-30T20:17:12.224064Z"
        },
        "papermill": {
          "duration": 0.735988,
          "end_time": "2024-11-30T20:17:12.226849",
          "exception": false,
          "start_time": "2024-11-30T20:17:11.490861",
          "status": "completed"
        },
        "tags": [],
        "id": "5599230a",
        "outputId": "5fc5c86b-d7dc-452a-85eb-17acaca6e8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.6224, recall=0.4621, fmeasure=0.5304\n",
            "rouge2: precision=0.2577, recall=0.1908, fmeasure=0.2193\n",
            "rougeL: precision=0.3163, recall=0.2348, fmeasure=0.2696\n",
            "BERTScore: Precision=0.7739, Recall=0.7870, F1=0.7804\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_4, response_4_rag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_4, response_4_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af4697e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:12.280199Z",
          "iopub.status.busy": "2024-11-30T20:17:12.279762Z",
          "iopub.status.idle": "2024-11-30T20:17:12.285606Z",
          "shell.execute_reply": "2024-11-30T20:17:12.284785Z"
        },
        "papermill": {
          "duration": 0.035062,
          "end_time": "2024-11-30T20:17:12.287205",
          "exception": false,
          "start_time": "2024-11-30T20:17:12.252143",
          "status": "completed"
        },
        "tags": [],
        "id": "2af4697e",
        "outputId": "029ac143-6b39-4554-fd51-006e611d6d24"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Equation (1),  ( | 1)=(:1 .1,. ),{1,,}, describes the forward diffusion process within Denoising Diffusion Probabilistic Models (DDPMs).  It's a Markov chain where Gaussian noise is iteratively added to the input data (x<sub>t-1</sub>) at each timestep (t). The noise amount is controlled by the variance schedule (<sub>t</sub>), creating a sequence of increasingly noisy latent states (x<sub>1</sub> to x<sub>T</sub>). The equation states that each noisy state x<sub>t</sub> is sampled from a normal distribution () with a mean dependent on the previous state (scaled by 1 ) and a variance (<sub>t</sub>I), where I is the identity matrix.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_4_arag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b0028ad",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:12.338399Z",
          "iopub.status.busy": "2024-11-30T20:17:12.338128Z",
          "iopub.status.idle": "2024-11-30T20:17:13.064655Z",
          "shell.execute_reply": "2024-11-30T20:17:13.063675Z"
        },
        "papermill": {
          "duration": 0.756232,
          "end_time": "2024-11-30T20:17:13.067880",
          "exception": false,
          "start_time": "2024-11-30T20:17:12.311648",
          "status": "completed"
        },
        "tags": [],
        "id": "2b0028ad",
        "outputId": "e6f4b1d9-4393-46c7-ee82-4ad4953a232b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.4865, recall=0.4091, fmeasure=0.4444\n",
            "rouge2: precision=0.1727, recall=0.1450, fmeasure=0.1577\n",
            "rougeL: precision=0.2523, recall=0.2121, fmeasure=0.2305\n",
            "BERTScore: Precision=0.7366, Recall=0.7771, F1=0.7563\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_4, response_4_arag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_4, response_4_arag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "324701c5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:13.165156Z",
          "iopub.status.busy": "2024-11-30T20:17:13.164820Z",
          "iopub.status.idle": "2024-11-30T20:17:13.170379Z",
          "shell.execute_reply": "2024-11-30T20:17:13.169535Z"
        },
        "papermill": {
          "duration": 0.06911,
          "end_time": "2024-11-30T20:17:13.173826",
          "exception": false,
          "start_time": "2024-11-30T20:17:13.104716",
          "status": "completed"
        },
        "tags": [],
        "id": "324701c5",
        "outputId": "b15fef6a-2995-406e-e063-4e4843380b7b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "ProteinBERT was developed by Brandes et al.  It is a specialized deep language model for protein sequences that combines local and global representations for comprehensive end-to-end processing.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_5_base))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6382758b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:13.262481Z",
          "iopub.status.busy": "2024-11-30T20:17:13.261695Z",
          "iopub.status.idle": "2024-11-30T20:17:14.143800Z",
          "shell.execute_reply": "2024-11-30T20:17:14.142777Z"
        },
        "papermill": {
          "duration": 0.93312,
          "end_time": "2024-11-30T20:17:14.145888",
          "exception": false,
          "start_time": "2024-11-30T20:17:13.212768",
          "status": "completed"
        },
        "tags": [],
        "id": "6382758b",
        "outputId": "853c4420-2b17-4a01-f79d-1661b8787eae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.9310, recall=0.9000, fmeasure=0.9153\n",
            "rouge2: precision=0.8214, recall=0.7931, fmeasure=0.8070\n",
            "rougeL: precision=0.9310, recall=0.9000, fmeasure=0.9153\n",
            "BERTScore: Precision=0.9656, Recall=0.9331, F1=0.9491\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_5, response_5_base)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_5, response_5_base)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28740b8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:14.199289Z",
          "iopub.status.busy": "2024-11-30T20:17:14.198272Z",
          "iopub.status.idle": "2024-11-30T20:17:14.204262Z",
          "shell.execute_reply": "2024-11-30T20:17:14.203426Z"
        },
        "papermill": {
          "duration": 0.034025,
          "end_time": "2024-11-30T20:17:14.206199",
          "exception": false,
          "start_time": "2024-11-30T20:17:14.172174",
          "status": "completed"
        },
        "tags": [],
        "id": "b28740b8",
        "outputId": "f025ae0b-5bf5-45eb-87d2-49f36e0e69ab"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Brandes et al. developed ProteinBERT. It's a specialized deep language model for protein sequences that uses both local and global representations for complete end-to-end processing.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_5_rag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d06794",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:14.256963Z",
          "iopub.status.busy": "2024-11-30T20:17:14.256468Z",
          "iopub.status.idle": "2024-11-30T20:17:14.990248Z",
          "shell.execute_reply": "2024-11-30T20:17:14.989256Z"
        },
        "papermill": {
          "duration": 0.761176,
          "end_time": "2024-11-30T20:17:14.991989",
          "exception": false,
          "start_time": "2024-11-30T20:17:14.230813",
          "status": "completed"
        },
        "tags": [],
        "id": "90d06794",
        "outputId": "ef5eb33e-4f4f-4bfc-87d1-5f8d8777e8a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.8929, recall=0.8333, fmeasure=0.8621\n",
            "rouge2: precision=0.7037, recall=0.6552, fmeasure=0.6786\n",
            "rougeL: precision=0.8214, recall=0.7667, fmeasure=0.7931\n",
            "BERTScore: Precision=0.9408, Recall=0.9086, F1=0.9245\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_5, response_5_rag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_5, response_5_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3403d2e8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:15.045289Z",
          "iopub.status.busy": "2024-11-30T20:17:15.044989Z",
          "iopub.status.idle": "2024-11-30T20:17:15.050510Z",
          "shell.execute_reply": "2024-11-30T20:17:15.049779Z"
        },
        "papermill": {
          "duration": 0.033451,
          "end_time": "2024-11-30T20:17:15.051973",
          "exception": false,
          "start_time": "2024-11-30T20:17:15.018522",
          "status": "completed"
        },
        "tags": [],
        "id": "3403d2e8",
        "outputId": "d2bf9933-0daa-4a35-fc0a-035d09bcf3d8"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Brandes et al. developed ProteinBERT.  It is a specialized deep language model for protein sequences that combines local and global representations for comprehensive end-to-end processing.\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(Markdown(response_5_arag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ed9b264",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-11-30T20:17:15.104656Z",
          "iopub.status.busy": "2024-11-30T20:17:15.103575Z",
          "iopub.status.idle": "2024-11-30T20:17:15.804639Z",
          "shell.execute_reply": "2024-11-30T20:17:15.803667Z"
        },
        "papermill": {
          "duration": 0.7289,
          "end_time": "2024-11-30T20:17:15.806582",
          "exception": false,
          "start_time": "2024-11-30T20:17:15.077682",
          "status": "completed"
        },
        "tags": [],
        "id": "6ed9b264",
        "outputId": "4acdd12e-24bd-4a79-d872-73103f7d3ca6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores:\n",
            "ROUGE Scores:\n",
            "rouge1: precision=0.9259, recall=0.8333, fmeasure=0.8772\n",
            "rouge2: precision=0.7308, recall=0.6552, fmeasure=0.6909\n",
            "rougeL: precision=0.8519, recall=0.7667, fmeasure=0.8070\n",
            "BERTScore: Precision=0.9472, Recall=0.9127, F1=0.9296\n"
          ]
        }
      ],
      "source": [
        "print(\"Calculating scores:\")\n",
        "rouge_scores = calculate_rouge_scores(right_answer_5, response_5_arag)\n",
        "bert_precision, bert_recall, bert_f1 = calculate_bert_score(right_answer_5, response_5_arag)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3dbe3c",
      "metadata": {
        "papermill": {
          "duration": 0.024556,
          "end_time": "2024-11-30T20:17:15.863529",
          "exception": false,
          "start_time": "2024-11-30T20:17:15.838973",
          "status": "completed"
        },
        "tags": [],
        "id": "0f3dbe3c"
      },
      "source": [
        "### Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ec5547b",
      "metadata": {
        "papermill": {
          "duration": 0.024974,
          "end_time": "2024-11-30T20:17:15.913266",
          "exception": false,
          "start_time": "2024-11-30T20:17:15.888292",
          "status": "completed"
        },
        "tags": [],
        "id": "8ec5547b"
      },
      "source": [
        "In this study, I stressed Gemini 1.5 Flash to chat directly with the paper vs a Naive RAG and an Advanced RAG for a knowledge Q&A on a review about the use of AI in Healthcare.\n",
        "The first considerations are oriented on the use case evaluation metrics.\n",
        "I haven't used Ragas, a library appropriately built for the RAG use case, because in showing three models, only two have the RAG architecture. Moreover, Ragas is born recently and I haven't seen Google documentation support.\n",
        "I used two metrics ROUGE and BERTScore.\n",
        "Looking at the results and focusing on the F1 score, it is clear that ROUGE is not fully suitable for the evaluation, unlike BERTScore which offers a better representation of the evaluation since the answers are overall correct.\n",
        "All models offer quite the same results, the advanced RAG may require better tuning and maybe a different embedding model, surely Gemini long text is an invaluable tool in competition with Retrieval-Augmented Generation.\n",
        "Though the dataset provided uses 1/3 of the suggested number of tokens, I think it's useful because it shows that Gemini 1.5 Flash for a text with almost 50 pages is competitive against RAG tools."
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "databundleVersionId": 9881586,
          "sourceId": 83735,
          "sourceType": "competition"
        },
        {
          "databundleVersionId": 10329619,
          "datasetId": 6139966,
          "sourceId": 10054929,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30787,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 184.926695,
      "end_time": "2024-11-30T20:17:18.838020",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-11-30T20:14:13.911325",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}